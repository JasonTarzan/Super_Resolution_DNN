{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Data Science Applications (2019/2020)\n",
    "## Final exam\n",
    "\n",
    "* **Student(s)**: Francesco Russo (1449025), Iason Tsardanidis (1846834), Michele Cernigliaro (1869097)\n",
    "* **Reference paper / topic**: Hu, Xiaodan, et al. \"RUNet: A Robust UNet Architecture for Image Super-Resolution.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Report\n",
    "\n",
    "##### 1.1) Robust UNet Architecture for Image Super-Resolution\n",
    "\n",
    "In this project we faced the so called *Single Image Super-Resolution* (SISR) problem, which aims to infer High-Resolution (HR) images from Low-Resolution (LR) ones. The approach used by Hu, Xiaodan et al. in [1] is to exploit the U-Net network used for segmentation tasks introduced by Ronneberger et al. in [2], ending up in a network which is capable to enhance image quality. The network is shown in the image below. \n",
    "\n",
    "\n",
    "![Robust-UNet architecture](https://raw.githubusercontent.com/cerniello/Super_Resolution_DNN/master/img/00_RUnet.png)\n",
    "\n",
    "It consists in two main parts: a **downsampling part**, composed by a series of blocks made up with convolutional layers, batch normalization and ReLU activation functions, and an **upsampling part**, where sub-pixel convolutional layers are used in order up-scale the <font color='red'>low-resolution filters obtained in the downsampling</font>. After each upsampling, a concatenation is performed. Unlike in U-Net, in RUnet we notice the presence of residual blocks in the downsampling part which as suggested in [1] \"allows the network to learn more complex structure\". In conclusion, in contrast to any of the common Super-Resolution network architectures the dimensions of the input and the output images coincide.\n",
    "\n",
    "# <font color='red'> Some interesting artifacts</font>\n",
    "\n",
    "##### 1.1) Pixel Shuffle\n",
    "\n",
    "![Robust-UNet architecture](https://raw.githubusercontent.com/cerniello/Super_Resolution_DNN/master/img/pixel_shuffle.jpeg)\n",
    "\n",
    "It is about a specific type of image reshaping in order to increase the resolution of the already *down-scaled* image. Preceded by a convolution layer, all together compose a **sub-pixel** layer. It is a *state-of-the-art* technique and very famous in SR tasks, since unlikely to ordinary upsampling layers, instead of putting zeros in between pixels and having to do extra computation, they calculate more convolutions in lower resolution and resize the resulting map combining existed filters into an upscaled image in order to avoid redundant zeros between layers. For more information check [3]. \n",
    "\n",
    "\n",
    "##### 1.2) Perceptual loss, metrics\n",
    "\n",
    "For the training of this network we will implement the **perceptual loss** suggested from the regarding paper implements as a very innovative strategy for SR problems (see also [4]). It is about a metric distance provided from pre-trained CNN layers between the output and the target image:\n",
    "\n",
    "$$ L^j = \\frac{1}{C^j H^j W^j}{||\\phi_j(Ground Truth) - \\phi_j(Predicted Image)||}^2_2$$\n",
    "\n",
    "\n",
    "where $C_j × H_j × W_j$ denotes the size of feature map ($\\phi_j$) obtained at the j th convolutional layer.\n",
    "\n",
    "\n",
    "Moreover for the evaluation of our results we will apply some metrics related to *image evaluation*:\n",
    "\n",
    "- **MSE (Mean-Squared-Error)**\n",
    "\n",
    "- **PSNR (Peak signal-to-noise ratio)**: An engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation (usually expressed in lagarithmic scale).\n",
    "\n",
    "- **SSIM (Structural Similarity)**: A perception-based model (not estimates absolute errors) that considers image degradation as perceived change in structural information.\n",
    "\n",
    "##### 1.3) Dataset\n",
    "\n",
    "\n",
    "\n",
    "##### 1.4) Implementation\n",
    "\n",
    "Todo:\n",
    "    - Environment: We will implend the task in Colab virtual_environment taking advantage of the available GPU units using Tensorflow and Keras.\n",
    "    - add smt related to blocks: kernel size, zero padding, ...\n",
    "    - add smt related to the problem of the # of filters in the residuals: we solve it adding a residual block with 1x1 convolution\n",
    "    - other problems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: External libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for Colab: tf2.1 and PyDrive to download the dataset\n",
    "\n",
    "# installing tensorflow 2.1\n",
    "!pip install --quiet tensorflow-gpu==2.1.0\n",
    "\n",
    "# After this, restart the runtime to avoid authentication issues\n",
    "!pip install -q -U PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyDrive and associated libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linnaeus_5 dataset from a Google Drive:\n",
    "file_id = '1hYf6wlpO0Kp0y_btm2PKmWmpX4Gpqkg3'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile('Linnaeus_5.zip')\n",
    "\n",
    "!ls\n",
    "!unzip -q Linnaeus_5.zip\n",
    "!ls Linnaeus_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "#from PIL import Image # in order to show the image\n",
    "import cv2            # in order to read image as np.array\n",
    "\n",
    "\n",
    "# vgg net for perceptual loss\n",
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the image input and our batch size\n",
    "input_size = 128 # 128x128x3 images\n",
    "batch_size= 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "=================================================================\n",
      "Total params: 260,160\n",
      "Trainable params: 0\n",
      "Non-trainable params: 260,160\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Perceptual loss function \n",
    "\n",
    "# load pre-trained (imagenet) vgg network, excluding fully-connected layer on the top\n",
    "vgg = VGG19(include_top=False, weights='imagenet', input_shape=(None,None,3))\n",
    "vgg_layer = K.Model(inputs=vgg.input, outputs=vgg.get_layer('block2_conv2').output)\n",
    "# make the net not trainable\n",
    "for l in vgg_layer.layers: l.trainable=False \n",
    "\n",
    "print(vgg_layer.summary())\n",
    "\n",
    "def perceptual_loss(y_true,y_pred):\n",
    "    '''This function computes the perceptual loss using an already trained VGG layer'''\n",
    "    y_t=vgg_layer(y_true)\n",
    "    y_p=vgg_layer(y_pred)\n",
    "    loss=K.losses.mean_squared_error(y_t,y_p)\n",
    "    loss = loss/(3*3*128) # dividing with the size of the output feature map\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel size: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "print('kernel size:', vgg_layer.get_layer(index=-1).kernel_size) #size of the output kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining other metrics:\n",
    "def psnr(y_true,y_pred):\n",
    "    return tf.image.psnr(y_true,y_pred,1.0)\n",
    "def ssim(y_true,y_pred):\n",
    "    return tf.image.ssim(y_true,y_pred,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, None, None, 3)     0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def image_proc_net(training_mode=True):\n",
    "    \"\"\"\n",
    "    Function which creates the model to preprocess images.\n",
    "    2 different types of model with different layers depending\n",
    "    on whether it is in training mode or in testing mode\n",
    "    \"\"\"\n",
    "    inputs = K.layers.Input((None, None, 3))\n",
    "    if training_mode:\n",
    "        # training mode: downsampling, gaussian noise and upsampling\n",
    "        x = K.layers.MaxPool2D(pool_size=(2,2))(inputs)\n",
    "        x = K.layers.GaussianNoise(5)(x)\n",
    "        x = K.layers.UpSampling2D((2,2))(x)\n",
    "    else:\n",
    "        # testing mode: just an upsampling\n",
    "        x = K.layers.UpSampling2D((2,2))(inputs)\n",
    "\n",
    "    model = K.models.Model(inputs, x)\n",
    "\n",
    "    for l in model.layers: l.trainable=False\n",
    "\n",
    "    return model\n",
    "\n",
    "# istantiating the two models (for train and test)\n",
    "image_proc_train = image_proc_net(training_mode=True)\n",
    "image_proc_test = image_proc_net(training_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, None, None, 3)     0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#training_mode\n",
    "print(image_proc_train.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, None, None, 3)     0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#testing_mode\n",
    "print(image_proc_test.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(image, training_mode=True):\n",
    "    \"\"\"\n",
    "    Function which preprocess automatically an image\n",
    "    wether it's in training or testing mode\n",
    "\n",
    "    input: \n",
    "      - image: np array (image tensor)\n",
    "      - training_mode: binary flag\n",
    "    \"\"\"\n",
    "    image = np.expand_dims(image,axis=0)\n",
    "    if training_mode:\n",
    "        return tf.squeeze(image_proc_train(image))\n",
    "    else:\n",
    "        return tf.squeeze(image_proc_test(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train, validation and test lists containing the image paths\n",
    "\n",
    "all_images = [] # all train image paths\n",
    "\n",
    "train_list = [] # train image path list after train-validation split\n",
    "val_list = []   # val   image path list after train-validation split\n",
    "\n",
    "# 128x128 in training phase\n",
    "path = 'Linnaeus_5/train_128'\n",
    " \n",
    "files = os.listdir(path)\n",
    "for file in files:\n",
    "    path_file = path+'/'+file\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path_file):\n",
    "        filenames = list(map(lambda x:path_file+'/'+x,filenames))\n",
    "        all_images.extend(filenames)\n",
    "\n",
    "\n",
    "all_images = all_images[:2000] # picking only 2000 over 6000 images\n",
    "np.random.shuffle(all_images)  # random shuffling\n",
    "\n",
    "# keep 85% for train and 15% for validation\n",
    "train_len = int(len(all_images)*0.85)\n",
    "\n",
    "train_list = all_images[:train_len]\n",
    "val_list = all_images[train_len:]\n",
    "\n",
    "\n",
    "# test\n",
    "# for the test set we opted to take the same dataset\n",
    "# but with images of size 64x64 instead of 128x128\n",
    "\n",
    "test_list = []\n",
    "\n",
    "path = 'Linnaeus_5/test_64'\n",
    "files = os.listdir(path)\n",
    "for file in files:\n",
    "    path_file = path+'/'+file\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path_file):\n",
    "        filenames = list(map(lambda x:path_file+'/'+x,filenames))\n",
    "        test_list.extend(filenames)\n",
    "\n",
    "np.random.shuffle(test_list) # random shuffling\n",
    "\n",
    "test_list = test_list[:500]\n",
    "\n",
    "print('images: ')\n",
    "print('train:', len(train_list), 'val:', len(val_list), 'test:', len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining train, validation and test generator\n",
    "\n",
    "def train_generator():\n",
    "    global batch_size\n",
    "    while True:\n",
    "        for start in range(0, len(train_list), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(train_list))\n",
    "                    ids_train_batch = train_list[start:end]\n",
    "                    for i,ids in enumerate(ids_train_batch):\n",
    "                        img_y = cv2.imread(ids)\n",
    "                        img_x = image_preprocess(img_y, training_mode=True)\n",
    "                        x_batch.append(np.array(img_x,np.float32)/255.)\n",
    "                        y_batch.append(np.array(img_y,np.float32)/255.)\n",
    "                    x_batch = np.array(x_batch)\n",
    "                    y_batch = np.array(y_batch)\n",
    "                    yield x_batch,y_batch\n",
    "                    \n",
    "def valid_generator():\n",
    "    global batch_size\n",
    "    while True:\n",
    "        for start in range(0, len(val_list), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(val_list))\n",
    "                    ids_val_batch = val_list[start:end]\n",
    "                    for i,ids in enumerate(ids_val_batch):\n",
    "                        img_y = cv2.imread(ids)\n",
    "                        img_x = image_preprocess(img_y, training_mode=True)\n",
    "                        x_batch.append(np.array(img_x,np.float32)/255.)\n",
    "                        y_batch.append(np.array(img_y,np.float32)/255.)\n",
    "                    x_batch = np.array(x_batch)\n",
    "                    y_batch = np.array(y_batch)\n",
    "                    yield x_batch,y_batch\n",
    "\n",
    "\n",
    "def test_generator():\n",
    "    global batch_size\n",
    "    while True:\n",
    "        for start in range(0, len(test_list), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(test_list))\n",
    "                    ids_test_batch = test_list[start:end]\n",
    "                    for i,ids in enumerate(ids_test_batch):\n",
    "                        img_y = cv2.imread(ids)\n",
    "                        img_x = image_preprocess(img_y, training_mode=False)\n",
    "                        x_batch.append(np.array(img_x,np.float32)/255.)\n",
    "                        y_batch.append(np.array(img_y,np.float32)/255.)\n",
    "                    x_batch = np.array(x_batch)\n",
    "                    y_batch = np.array(y_batch)\n",
    "                    yield x_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel shuffle \n",
    "def pixel_shuffle(scale):\n",
    "    '''\n",
    "    This function implements pixel shuffling.\n",
    "    ATTENTION: the scale should be bigger than 2, otherwise just returns the input.\n",
    "    '''\n",
    "    if scale > 1:\n",
    "        return lambda x: tf.nn.depth_to_space(x, scale)\n",
    "    else:\n",
    "        return lambda x:x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blocks definition for upscaling/downscaling\n",
    "\n",
    "def add_down_block(x_inp, filters, kernel_size=(3, 3), padding=\"same\", strides=1,r=False):\n",
    "    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x_inp)\n",
    "    x = K.layers.BatchNormalization()(x)\n",
    "    x = K.layers.Activation('relu')(x)\n",
    "    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    x = K.layers.BatchNormalization()(x)\n",
    "    if r:\n",
    "        # if r=True then we import an (1X1) Conv2D after input layer \n",
    "        # in order the dimensions of 2 tensors coincide.\n",
    "        x_inp = K.layers.Conv2D(filters,(1,1), padding=padding, strides=strides)(x_inp)\n",
    "    x = K.layers.Add()([x,x_inp])\n",
    "    return x\n",
    "\n",
    "def add_up_block(x_inp,skip,filters, kernel_size=(3, 3), padding=\"same\", strides=1,upscale_factor=2):\n",
    "    x = pixel_shuffle(scale=upscale_factor)(x_inp)\n",
    "    x = K.layers.Concatenate()([x, skip])\n",
    "    x = K.layers.BatchNormalization()(x)\n",
    "    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    x = K.layers.Activation('relu')(x)\n",
    "    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    x = K.layers.Activation('relu')(x)\n",
    "    x = K.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def add_bottleneck(x_inp,filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x_inp)\n",
    "    x = K.layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUNet():\n",
    "    \"\"\"\n",
    "      Implementing with Keras the Robust UNet Architecture as proposed by\n",
    "      Xiaodan Hu, Mohamed A. Naiel, Alexander Wong, Mark Lamm, Paul Fieguth\n",
    "      in \"RUNet: A Robust UNet Architecture for Image Super-Resolution\"\n",
    "    \"\"\"\n",
    "    inputs = K.layers.Input((input_size,input_size, 3))\n",
    "    \n",
    "    \n",
    "    down_1 = K.layers.Conv2D(64,(7,7), padding=\"same\", strides=1)(inputs)\n",
    "    down_1 = K.layers.BatchNormalization()(down_1)\n",
    "    down_1 = K.layers.Activation('relu')(down_1)\n",
    "    \n",
    "    down_2 = K.layers.MaxPool2D(pool_size=(2,2))(down_1)\n",
    "    down_2 = add_down_block(down_2,64)\n",
    "    down_2 = add_down_block(down_2,64)\n",
    "    down_2 = add_down_block(down_2,64)\n",
    "    down_2 = add_down_block(down_2,128,r=True)\n",
    "    \n",
    "    down_3 = K.layers.MaxPool2D(pool_size=(2, 2),strides=2)(down_2)\n",
    "    down_3 = add_down_block(down_3,128)\n",
    "    down_3 = add_down_block(down_3,128)\n",
    "    down_3 = add_down_block(down_3,128)\n",
    "    down_3 = add_down_block(down_3,256,r=True)\n",
    "    \n",
    "    down_4 = K.layers.MaxPool2D(pool_size=(2, 2))(down_3)\n",
    "    down_4 = add_down_block(down_4,256)\n",
    "    down_4 = add_down_block(down_4,256)\n",
    "    down_4 = add_down_block(down_4,256)\n",
    "    down_4 = add_down_block(down_4,256)\n",
    "    down_4 = add_down_block(down_4,512,r=True) \n",
    "    \n",
    "    down_5 = K.layers.MaxPool2D(pool_size=(2, 2))(down_4)\n",
    "    down_5 = add_down_block(down_5,512)\n",
    "    down_5 = add_down_block(down_5,512)\n",
    "    down_5 = K.layers.BatchNormalization()(down_5)\n",
    "    down_5 = K.layers.Activation('relu')(down_5)\n",
    "    \n",
    "    \n",
    "    bn_1 = add_bottleneck(down_5, 1024)\n",
    "    bn_2 = add_bottleneck(bn_1, 512)\n",
    "    \n",
    "    up_1 = add_up_block(bn_2,down_5, 512,upscale_factor=1)\n",
    "    up_2 = add_up_block(up_1,down_4, 384,upscale_factor=2)\n",
    "    up_3 = add_up_block(up_2,down_3, 256,upscale_factor=2)\n",
    "    up_4 = add_up_block(up_3,down_2, 96,upscale_factor=2) \n",
    "    \n",
    "    up_5 = pixel_shuffle(scale=2)(up_4)\n",
    "    up_5 = K.layers.Concatenate()([up_5,down_1])\n",
    "    up_5 = K.layers.Conv2D(99,(3,3), padding=\"same\", strides=1)(up_5)\n",
    "    up_5 = K.layers.Activation('relu')(up_5)\n",
    "    up_5 = K.layers.Conv2D(99,(3,3), padding=\"same\", strides=1)(up_5)\n",
    "    up_5 = K.layers.Activation('relu')(up_5)\n",
    "   \n",
    "    outputs = K.layers.Conv2D(3,(1,1), padding=\"same\")(up_5)\n",
    "    model = K.models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = RUNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=K.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,loss=perceptual_loss,metrics=[psnr,ssim,K.losses.mean_squared_error])\n",
    "history = model.fit_generator(generator=train_generator(),\n",
    "                              steps_per_epoch=np.ceil(float(len(train_list)) / float(batch_size)),\n",
    "                              epochs=10,\n",
    "                              verbose=1,\n",
    "                              validation_data=valid_generator(),\n",
    "                              shuffle=True,\n",
    "                              validation_steps=np.ceil(float(len(val_list)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Experimental evaluation\n",
    "\n",
    "*Almost done! Now you just need to convince me your code is working correctly.*\n",
    "\n",
    "**DO**:\n",
    "+ Provide a brief explanation of every experiment.\n",
    "+ All plots must *always* have self-explicative labels and captions.\n",
    "+ For long training times (e.g., > 1h), provide a comment and possibly a simplified set-up and/or a pre-trained model.\n",
    "\n",
    "**DON'T**:\n",
    "+ Assume by default I know what you are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_results(history, par1='loss', par2='val_loss', title='loss'):\n",
    "    \"\"\"\n",
    "    Plot the history of the the 2 metrics (par1, par2) during \n",
    "    the training (epochs)\n",
    "    \"\"\"\n",
    "    plt.plot(history.history[par1])\n",
    "    plt.plot(history.history[par2])\n",
    "    plt.title(title)\n",
    "    plt.ylabel(par1)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_results(history, 'psnr', 'val_psnr', 'psnr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_results(history, 'ssim', 'val_ssim', 'ssim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_results(history, 'mean_squared_error', 'val_mean_squared_error', 'means_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing some visual results of Super-Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pictures(img_idx, x_batch, y_batch):\n",
    "    \"\"\"\n",
    "    Function which shows 3 images:\n",
    "    - Ground truth: High Resolution image\n",
    "    - Low Resolution image\n",
    "    - Super Resolution image using our trained model\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15,18))\n",
    "\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    im = model(np.expand_dims(x_batch[img_idx],axis=0))\n",
    "    im = np.squeeze(im)\n",
    "    ax1.imshow(abs(im))\n",
    "    ax1.set_title('Super Resolution (from LR)')\n",
    "\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax2.imshow(x_batch[img_idx])\n",
    "    ax2.set_title('Low Resolution')\n",
    "\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax3.imshow(y_batch[img_idx])\n",
    "    ax3.set_title('Ground truth')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to visualize the results with some training images\n",
    "\n",
    "batch_size=10\n",
    "for start in range(0, len(train_list), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(train_list))\n",
    "            ids_train_batch = train_list[start:end]\n",
    "            for i,ids in enumerate(ids_train_batch):\n",
    "                img_y = cv2.imread(ids)\n",
    "                img_x = image_preprocess(img_y)\n",
    "                x_batch.append(np.array(img_x,np.float32)/255.)\n",
    "                y_batch.append(np.array(img_y,np.float32)/255.)\n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = np.array(y_batch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pictures(0, x_batch, y_batch)\n",
    "show_pictures(5, x_batch, y_batch)\n",
    "show_pictures(6, x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with the Linnaeus_5 test set of 64x64 images\n",
    "# and comparing the results with their relative HR images (128x128)\n",
    "\n",
    "batch_size=10\n",
    "for start in range(0, len(test_list), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(test_list))\n",
    "            ids_test_batch = test_list[start:end]\n",
    "            for i,ids in enumerate(ids_test_batch):\n",
    "                # taking the ids (filepath) of 64x64 img and obtain its 128x128 img\n",
    "                ids_128 = '128'.join(ids.split('64'))\n",
    "                img_x = cv2.imread(ids)     # x: 64x64 img\n",
    "                img_x = image_preprocess(img_x, training_mode=False)\n",
    "                img_y = cv2.imread(ids_128) # y: 128x128 img \n",
    "\n",
    "                x_batch.append(np.array(img_x,np.float32)/255.)\n",
    "                y_batch.append(np.array(img_y,np.float32)/255.)\n",
    "\n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = np.array(y_batch)\n",
    "\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9baac8e2c40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_batch' is not defined"
     ]
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pictures(0, x_batch, y_batch)\n",
    "show_pictures(4, x_batch, y_batch)\n",
    "show_pictures(2, x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "\n",
    "#### - Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Hu, Xiaodan, et al. \"RUNet: A Robust UNet Architecture for Image Super-Resolution.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2019.\n",
    "\n",
    "[2] Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas. \"U-Net: Convolutional Networks for Biomedical Image Segmentation\". Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015.\n",
    "\n",
    "[3] Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, Zehan Wang. \"Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\".\n",
    "\n",
    "[4] Justin Johnson, Alexandre Alahi, Li Fei-Fei. \"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
